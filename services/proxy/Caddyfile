# Caddy Configuration for Tale Platform
# Single entry point for both local development and production
#
# Supports blue-green deployments with automatic health-based routing
#
# TLS Configuration (environment variables):
# - TLS_MODE: Certificate mode
#   - "selfsigned" (default): Self-signed certificates via Caddy's internal CA
#   - "letsencrypt": Free trusted certificates from Let's Encrypt
#
# - TLS_EMAIL: Email for Let's Encrypt notifications (optional but recommended)
#
# Users always access via HTTPS regardless of TLS mode.
#
# Development: https://tale.local (TLS_MODE=selfsigned)
# Production:  https://yourdomain.com (TLS_MODE=letsencrypt)
#
# Blue-Green Deployment:
# - Both platform-blue:3000 and platform-green:3000 are listed as upstreams
# - Caddy automatically routes to healthy backends based on health checks
# - During deployment, traffic seamlessly switches to the healthy version

# Global options
{
	default_sni {$HOST:tale.local}
}

# HTTP health check endpoint (for Docker healthcheck)
# This avoids SSL issues when checking from localhost
:80 {
	handle /health {
		respond "OK" 200
	}
	# Redirect all other HTTP traffic to HTTPS
	handle {
		redir https://{host}{uri} permanent
	}
}

{$SITE_URL:https://tale.local} {
	# TLS Configuration - This line is replaced by entrypoint based on TLS_MODE
	# DO NOT MODIFY - entrypoint will replace this placeholder
	# TLS_PLACEHOLDER

	log {
		output stdout
		format console
		level INFO
	}

	# Health check endpoint for the proxy itself
	# Returns 200 OK if Caddy is running and able to serve requests
	# Usage: curl -k https://tale.local/health
	handle /health {
		respond "OK" 200
	}

	# Blue-Green Upstream Pool
	# Both backends are listed; Caddy routes to healthy ones based on health checks
	# For local development without blue-green, "platform:3000" works as fallback
	reverse_proxy {
		# Upstream backends - order matters for lb_policy first
		# Blue is preferred when both are healthy (listed first)
		to platform-blue:3000 platform-green:3000 platform:3000

		# Load balancing policy: use first healthy backend
		# This ensures deterministic behavior during deployments
		lb_policy first

		# Active health checking - Caddy probes these endpoints
		health_uri /api/health
		health_interval 2s
		health_timeout 2s
		health_status 200

		# CRITICAL for zero-downtime: backends must pass health checks before receiving traffic
		# health_passes: require 2 consecutive successful checks before marking healthy
		# This prevents routing to backends that just started but aren't ready yet
		health_passes 2

		# Passive health checking (circuit breaker)
		# Mark backend unhealthy after consecutive failures
		fail_duration 10s
		max_fails 2
		unhealthy_status 500 502 503 504

		# Retry on failure - try other healthy backends
		lb_try_duration 5s
		lb_try_interval 250ms

		# Headers
		header_up Host {host}
		header_up X-Real-IP {remote_host}
		header_up X-Forwarded-For {remote_host}
		header_up X-Forwarded-Proto {scheme}
		# Remove WebSocket compression extension to prevent "Invalid frame header" errors
		header_up -Sec-WebSocket-Extensions
	}

	# Encode responses, but skip WebSocket endpoints
	@notWebSocket {
		not path /api/*/sync
		not path /ws_api/*
	}
	encode @notWebSocket gzip zstd

	# Maintenance page when no healthy backends available
	# During blue-green deployments, there's a brief window (~8-10s) when
	# the old backend loses its database lease and the new one isn't ready yet.
	# This shows a friendly loading page instead of a raw 503 error.
	handle_errors 502 503 504 {
		root * /var/www
		rewrite * /maintenance.html
		file_server
	}

	header {
		-Server
		X-Content-Type-Options "nosniff"
		X-Frame-Options "SAMEORIGIN"
		Referrer-Policy "strict-origin-when-cross-origin"
	}

	# Only set HSTS for HTTPS connections
	@https {
		protocol https
	}
	header @https Strict-Transport-Security "max-age=31536000; includeSubDomains; preload"
}
