# =============================================================================
# LOCAL DEVELOPMENT ONLY
# =============================================================================
# This compose file is for LOCAL DEVELOPMENT only. It exposes ports that should
# NEVER be exposed in production (5432, 6379, 8001-8003).
#
# For PRODUCTION deployments, use the tale-deploy CLI:
#   tale-deploy deploy <version>
#
# The CLI generates secure compose configs inline with only ports 80/443 exposed.
# =============================================================================
#
# Docker Compose configuration for Tale Services
# This file defines the services needed to run the Tale platform
#
# Usage modes:
#   Development (default): docker compose up
#     - Builds images locally from Dockerfiles
#     - Uses PULL_POLICY=build (default)
#
#   Production: Use tale-deploy CLI instead (see above)
#
# Environment variables:
#   PULL_POLICY: 'build' (default) or 'always'
#   VERSION: Image tag to pull (default: 'latest')

services:
  # ============================================================================
  # Tale DB (TimescaleDB)
  # ============================================================================
  db:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-db:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/db/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # Container name
    container_name: tale-db

    # Port mapping: host:container
    # Access PostgreSQL at localhost:5432
    ports:
      - '5432:5432'

    # Volume mounts
    # Persist database data
    volumes:
      # PostgreSQL data directory
      - db-data:/var/lib/postgresql/data
      # Backup directory (optional)
      - db-backup:/var/lib/postgresql/backup

    # Environment file
    # Create a .env file in the project root with your database credentials
    env_file:
      - .env

    # Restart policy
    # Automatically restart the container if it crashes
    restart: unless-stopped

    # Health check
    # Docker will check if the database is ready
    healthcheck:
      test:
        [
          'CMD-SHELL',
          'pg_isready -U ${POSTGRES_USER:-tale} -d ${POSTGRES_DB:-tale}',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Resource limits (optional, adjust based on your needs)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'
    #       memory: 8G
    #     reservations:
    #       cpus: '2'
    #       memory: 4G

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      - internal

  # ============================================================================
  # Tale Crawler (Crawl4AI)
  # ============================================================================
  # Stateless service - no container_name for blue-green deployment
  crawler:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-crawler:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/crawler/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # Port mapping: host:container (for development)
    # Access Crawler API at localhost:8002
    ports:
      - '8002:8002'

    # Environment file
    # Create a .env file in the project root with your configuration
    env_file:
      - .env

    # Restart policy
    # Automatically restart the container if it crashes
    restart: unless-stopped

    # Health check - optimized for faster blue-green deployment
    # Docker will check if the service is healthy
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8002/health']
      interval: 5s
      timeout: 3s
      retries: 2
      start_period: 40s

    # Resource limits (optional, adjust based on your needs)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: 4G
    #     reservations:
    #       cpus: '1'
    #       memory: 2G

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      - internal

  # ============================================================================
  # Tale Graph DB (FalkorDB)
  # ============================================================================
  # Redis-based graph database optimized for GraphRAG
  # - Native multi-tenant support (10K+ graphs per instance)
  # - Low latency (~140ms p99)
  # - Client-server architecture (no file-level locking issues)
  # - Combined graph and vector storage via hybrid adapter
  # Replaces embedded Kuzu (archived) and LanceDB
  graph-db:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-graph-db:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/graph-db/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # Container name
    container_name: tale-graph-db

    # Port mapping: host:container
    # Access FalkorDB at localhost:6379 (Redis protocol)
    # Access FalkorDB Browser UI at localhost:6380 (mapped from internal 3000)
    ports:
      - '6379:6379'
      - '6380:3000'

    # Volume mounts
    # Persist graph and vector data
    # FalkorDB stores data in /var/lib/falkordb/data (not /data)
    volumes:
      - graph-db-data:/var/lib/falkordb/data

    # Environment file
    env_file:
      - .env

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 5s
      retries: 3

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      - internal

  # ============================================================================
  # Tale RAG (Cognee)
  # ============================================================================
  # Stateless service - no container_name for blue-green deployment
  rag:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-rag:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/rag/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # Port mapping: host:container (for development)
    # Access RAG API at localhost:8001
    ports:
      - '8001:8001'

    # Volume mounts
    # Persist cognee data
    volumes:
      # Cognee data directory
      - rag-data:/app/data

    # Environment file
    # Create a .env file in the project root with your API keys
    env_file:
      - .env

    # Restart policy
    # Automatically restart the container if it crashes
    restart: unless-stopped

    # Health check - optimized for faster blue-green deployment
    # Docker will check if the service is healthy
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8001/health']
      interval: 5s
      timeout: 3s
      retries: 2
      start_period: 40s

    # Dependencies
    # Wait for database and graph-db (FalkorDB) to be ready
    depends_on:
      - db
      - graph-db

    # Resource limits (optional, adjust based on your needs)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: 4G
    #     reservations:
    #       cpus: '1'
    #       memory: 2G

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      - internal

  # ============================================================================
  # Tale Platform (TanStack Start + Convex)
  # ============================================================================
  # Stateless service - no container_name for blue-green deployment
  platform:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-platform:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/platform/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # No port mapping needed - accessed via proxy at https://tale.local (or your domain)
    # internally via http://platform:3000

    # Volume mounts
    # Persist Convex local data
    volumes:
      # Convex local backend data
      - platform-convex-data:/app/convex-data
      # Mount Caddy's CA certificates for trusting self-signed certificates (development only)
      - caddy-data:/caddy-data:ro

    # Environment file
    # Create a .env file in the project root with your configuration
    env_file:
      - .env

    # Restart policy
    # Automatically restart the container if it crashes
    restart: unless-stopped

    # Health check - optimized for faster blue-green deployment
    # Docker will check if the service is healthy (both TanStack Start and Convex)
    healthcheck:
      test:
        [
          'CMD-SHELL',
          'curl -sf http://localhost:3000/api/health && curl -sf http://localhost:3210/version',
        ]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 120s

    # Dependencies
    # Wait for backend services to be ready
    depends_on:
      - db
      - rag
      - crawler
      - search

    # Resource limits (optional, adjust based on your needs)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: 4G
    #     reservations:
    #       cpus: '1'
    #       memory: 2G

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      - internal

  # ============================================================================
  # Tale Search (SearXNG) - Self-hosted Meta Search Engine
  # ============================================================================
  # Stateless service - no container_name for blue-green deployment
  search:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-search:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/search/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # Port mapping: host:container (for development)
    # Access SearXNG at localhost:8003
    ports:
      - '8003:8080'

    # Environment file
    env_file:
      - .env

    # Restart policy
    restart: unless-stopped

    # Health check - optimized for faster blue-green deployment
    healthcheck:
      test:
        [
          'CMD',
          'wget',
          '--no-verbose',
          '--tries=1',
          '--spider',
          '--header=X-Real-IP: 127.0.0.1',
          'http://localhost:8080/healthz',
        ]
      interval: 5s
      timeout: 3s
      retries: 2
      start_period: 30s

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      - internal

  # ============================================================================
  # Tale Proxy (Caddy)
  # ============================================================================
  # Single entry point for all traffic (both local dev and production)
  # Always serves over HTTPS with configurable certificate source.
  #
  # TLS Configuration (set in .env):
  #   TLS_MODE=selfsigned   - Self-signed certs (default, for development)
  #   TLS_MODE=letsencrypt  - Let's Encrypt certs (for production)
  #
  # For local development:
  #   - Run: docker compose up
  #   - Access: https://tale.local (self-signed cert, browser warning expected)
  #   - To trust certs: docker exec tale-proxy caddy trust
  #
  # For production with HTTPS:
  #   - Set HOST=yourdomain.com and SITE_URL=https://yourdomain.com in .env
  #   - Set TLS_MODE=letsencrypt
  #   - Access: https://yourdomain.com (trusted Let's Encrypt cert)
  #
  proxy:
    # Image from GHCR (used when PULL_POLICY=always)
    image: ghcr.io/tale-project/tale/tale-proxy:${VERSION:-latest}
    # Pull policy: 'build' for local dev, 'always' for production
    pull_policy: ${PULL_POLICY:-build}
    # Build configuration (used when PULL_POLICY=build)
    build:
      context: .
      dockerfile: services/proxy/Dockerfile
      args:
        VERSION: ${VERSION:-dev}

    # Container name
    container_name: tale-proxy

    # Port mapping: host:container
    # Access the platform via HTTP (80) or HTTPS (443)
    ports:
      - '80:80'
      - '443:443'

    # Volume mounts
    # Persist Caddy data (certificates, etc.)
    volumes:
      # Caddy data directory for certificates and configuration
      - caddy-data:/data
      # Caddy config directory
      - caddy-config:/config

    # Environment file
    # Create a .env file in the project root with your domain configuration
    env_file:
      - .env

    # Restart policy
    # Automatically restart the container if it crashes
    restart: unless-stopped

    # Health check
    # Docker will check if the proxy is healthy via /health endpoint on HTTP
    # Using HTTP on localhost avoids SSL/IPv6 issues for internal health checks
    healthcheck:
      test:
        [
          'CMD',
          'wget',
          '--no-verbose',
          '--tries=1',
          '--spider',
          'http://127.0.0.1:80/health',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

    # Note: We removed depends_on for platform to avoid circular dependency
    # when platform uses links/extra_hosts to reach localhost via proxy.
    # Caddy handles upstream unavailability gracefully.

    # Logging configuration
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

    # Networks
    networks:
      internal:
        # Hairpin NAT resolution via network alias
        # This allows containers to resolve HOST to the proxy container
        # via Docker's internal DNS, solving the hairpin NAT issue.
        # Use app.localhost (or another real hostname) instead of localhost
        # because localhost is always in /etc/hosts and can't be overridden.
        aliases:
          - ${HOST:-tale.local}

# ============================================================================
# Volumes
# ============================================================================
volumes:
  # Tale DB persistent storage
  db-data:
    driver: local
    # Optional: Use named volume with specific driver options
    # driver_opts:
    #   type: none
    #   o: bind
    #   device: /path/to/db/data

  # Tale DB backup storage
  db-backup:
    driver: local

  # Persistent storage for RAG service data (temp files, document processing)
  rag-data:
    driver: local

  # Persistent storage for graph-db (FalkorDB graph + vector data)
  graph-db-data:
    driver: local

  # Persistent storage for Platform Convex local backend
  platform-convex-data:
    driver: local

  # Persistent storage for Caddy proxy data (certificates, etc.)
  caddy-data:
    driver: local

  # Persistent storage for Caddy proxy configuration
  caddy-config:
    driver: local

# ============================================================================
# Networks
# ============================================================================
networks:
  # Internal network for Tale services
  internal:
    driver: bridge
